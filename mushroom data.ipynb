{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, value=None, children=None, label=None):\n",
    "        # Initializing a Node object with optional parameters:\n",
    "        # - feature: Storing the feature used for splitting in this node.\n",
    "        # - value: Storing the specific value used for splitting in this node (for categorical features).\n",
    "        # - children: A dictionary to store child nodes (subtrees).\n",
    "        # - label: Storing the predicted label for leaf nodes.\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.children = children if children is not None else {}\n",
    "        self.label = label\n",
    "class ID3DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        # Initializing an ID3 Decision Tree Classifier with an optional maximum depth parameter.\n",
    "        # - max_depth: Limiting the depth of the decision tree.\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None  # Initializing the decision tree as None.\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Fitting the decision tree classifier to the provided data.\n",
    "        # - X: The feature matrix.\n",
    "        # - y: The target labels.\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "        # Building the decision tree recursively starting with depth 0.\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Attempting to build a decision tree node recursively:\n",
    "        # - X: The feature matrix for the current node.\n",
    "        # - y: The target labels for the current node.\n",
    "        # - depth: The current depth in the tree.\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        majority_class = unique_classes[np.argmax(counts)]\n",
    "        # Finding the majority class among the target labels.\n",
    "        # Attempting to stop the recursion:\n",
    "        if len(unique_classes) == 1 or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return Node(label=majority_class)\n",
    "        # If all labels are the same or the maximum depth is reached, create a leaf node with the majority class.\n",
    "        best_feature, best_value, is_categorical = self._find_best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return Node(label=majority_class)\n",
    "        # Finding the best feature and value to split the data.\n",
    "        children = {}\n",
    "        if is_categorical:\n",
    "            unique_values = np.unique(X[:, best_feature])\n",
    "            for value in unique_values:\n",
    "                child_X, child_y = self._split_data_categorical(X, y, best_feature, value)\n",
    "                children[value] = self._build_tree(child_X, child_y, depth + 1)\n",
    "        else:\n",
    "            child_X_left, child_y_left, child_X_right, child_y_right = self._split_data_numerical(X, y, best_feature, best_value)\n",
    "            children['<= ' + str(best_value)] = self._build_tree(child_X_left, child_y_left, depth + 1)\n",
    "            children['> ' + str(best_value)] = self._build_tree(child_X_right, child_y_right, depth + 1)\n",
    "        # Recursively building child nodes for categorical and numerical features.\n",
    "        return Node(feature=best_feature, value=best_value, children=children)\n",
    "    def _find_best_split(self, X, y):\n",
    "        # Attempting to find the best feature and value to split the data:\n",
    "        # - X: The feature matrix.\n",
    "        # - y: The target labels.\n",
    "        num_samples, num_features = X.shape\n",
    "        entropy = self._calculate_entropy(y)\n",
    "        best_information_gain = -1\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        is_categorical = False\n",
    "        for feature in range(num_features):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            if len(unique_values) <= 1:\n",
    "                continue\n",
    "            if all(isinstance(value, (int, float)) for value in unique_values):\n",
    "                # Trying to find the best split for numerical features:\n",
    "                for value in unique_values:\n",
    "                    child_X_left, child_y_left, child_X_right, child_y_right = self._split_data_numerical(X, y, feature, value)\n",
    "                    information_gain = entropy - (len(child_y_left) / num_samples) * self._calculate_entropy(child_y_left) - (len(child_y_right) / num_samples) * self._calculate_entropy(child_y_right)\n",
    "                    if information_gain > best_information_gain:\n",
    "                        best_information_gain = information_gain\n",
    "                        best_feature = feature\n",
    "                        best_value = value\n",
    "                        is_categorical = False\n",
    "            else:\n",
    "                # Trying to find the best split for categorical features:\n",
    "                for value in unique_values:\n",
    "                    child_X, child_y = self._split_data_categorical(X, y, feature, value)\n",
    "                    information_gain = entropy - (len(child_y) / num_samples) * self._calculate_entropy(child_y)\n",
    "                    if information_gain > best_information_gain:\n",
    "                        best_information_gain = information_gain\n",
    "                        best_feature = feature\n",
    "                        best_value = value\n",
    "                        is_categorical = True\n",
    "\n",
    "        return best_feature, best_value, is_categorical\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        # Calculating the entropy of a set of target labels:\n",
    "        # - y: The target labels.\n",
    "\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "    def _split_data_numerical(self, X, y, feature, value):\n",
    "        # Attempting to split data for numerical features:\n",
    "        # - X: The feature matrix.\n",
    "        # - y: The target labels.\n",
    "        # - feature: The feature to split.\n",
    "        # - value: The value to split at.\n",
    "        left_mask = X[:, feature] <= value\n",
    "        right_mask = X[:, feature] > value\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "    def _split_data_categorical(self, X, y, feature, value):\n",
    "        # Attempting to split data for categorical features:\n",
    "        # - X: The feature matrix.\n",
    "        # - y: The target labels.\n",
    "        # - feature: The feature to split.\n",
    "        # - value: The value to split at.\n",
    "        mask = X[:, feature] == value\n",
    "        return X[mask], y[mask]\n",
    "    def predict(self, X):\n",
    "        # Predicting labels for a set of samples:\n",
    "        # - X: The feature matrix for prediction.\n",
    "        return np.array([self._predict_sample(x) for x in X])\n",
    "    def _predict_sample(self, x):\n",
    "        # Attempting to predict the label for a single sample:\n",
    "        # - x: The feature vector for prediction.\n",
    "        node = self.tree\n",
    "        while node.children:\n",
    "            feature_value = x[node.feature]\n",
    "            if feature_value in node.children:\n",
    "                node = node.children[feature_value]\n",
    "            else:\n",
    "                break\n",
    "        return node.label if node.label is not None else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory:  C:\\Users\\User\\Downloads\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = r\"C:\\Users\\User\\Downloads\"\n",
    "os.chdir(path)\n",
    "print(\"Current Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 1, Accuracy: 0.5612307692307692\n",
      "Max Depth: 2, Accuracy: 0.6184615384615385\n",
      "Max Depth: 3, Accuracy: 0.8061538461538461\n",
      "Max Depth: 4, Accuracy: 0.9766153846153847\n",
      "Max Depth: 5, Accuracy: 0.9969230769230769\n",
      "Max Depth: 6, Accuracy: 0.9969230769230769\n",
      "Max Depth: 7, Accuracy: 0.9987692307692307\n",
      "Max Depth: 8, Accuracy: 0.9993846153846154\n",
      "Max Depth: 9, Accuracy: 0.9993846153846154\n",
      "Max Depth: 10, Accuracy: 0.9993846153846154\n"
     ]
    }
   ],
   "source": [
    "# Attempting to read the Mushroom dataset from an Excel file named 'agricus.xlsx'.\n",
    "# The pandas library is being utilized for this operation, with 'pd' as its alias.\n",
    "data = pd.read_excel(\"agricus.xlsx\")\n",
    "\n",
    "# Putting effort to handle any missing values that might be present in the dataset.\n",
    "# It is known that the Mushroom dataset may contain missing values denoted as '?'.\n",
    "# Replacing these '?' symbols with None (which is treated as NaN in pandas) to facilitate further processing.\n",
    "# This step is crucial as handling missing data is essential for accurate analysis.\n",
    "data = data.replace(\"?\", None)\n",
    "\n",
    "# Trying to impute the missing values with the mode (most frequent value) for each column.\n",
    "# This approach is one of the standard methods for dealing with missing data,\n",
    "# especially when deleting the missing data is not a viable option.\n",
    "data = data.fillna(data.mode().iloc[0])\n",
    "\n",
    "# Attempting to convert all categorical features in the dataset into numerical values.\n",
    "# In the Mushroom dataset, all columns are considered categorical.\n",
    "# This conversion is necessary as many machine learning algorithms require numerical input.\n",
    "categorical_columns = data.columns\n",
    "for column in categorical_columns:\n",
    "    data[column] = data[column].astype('category').cat.codes\n",
    "\n",
    "# Splitting the dataset into features (X) and labels (y).\n",
    "# In the Mushroom dataset, the first column is typically used as the class label.\n",
    "# This separation is essential for supervised learning, where the algorithm learns from the features to predict the labels.\n",
    "X = data.iloc[:, 1:].values  # Extracting features: all columns except the first one\n",
    "y = data.iloc[:, 0].values   # Extracting labels: the first column\n",
    "\n",
    "# Dividing the data into training and testing sets.\n",
    "# This split is crucial for evaluating the model's performance on unseen data.\n",
    "# The 'train_test_split' function from sklearn is being used here,\n",
    "# setting aside 20% of the data for testing (test_size=0.2).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing a list of max_depth values to evaluate.\n",
    "# These values determine the maximum depth of the decision tree.\n",
    "# Experimenting with different depths can help in understanding the effect of depth on model performance.\n",
    "max_depth_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Initializing a list to store the accuracy for each max_depth value.\n",
    "# Accuracy is a common metric for evaluating classification models.\n",
    "accuracies = []\n",
    "\n",
    "# Looping over the different max_depth values.\n",
    "# For each value, a decision tree classifier is being trained and its accuracy is evaluated.\n",
    "for max_depth in max_depth_values:\n",
    "    classifier = ID3DecisionTreeClassifier(max_depth=max_depth)\n",
    "    classifier.fit(X_train, y_train)  # Training the classifier with the training set\n",
    "    y_pred = classifier.predict(X_test)  # Predicting labels for the test set\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculating the accuracy of the predictions\n",
    "    accuracies.append(accuracy)  # Appending the calculated accuracy to the list\n",
    "\n",
    "# Printing the max_depth values alongside their corresponding accuracies.\n",
    "# This output is helpful in determining the optimal depth for the decision tree.\n",
    "for max_depth, accuracy in zip(max_depth_values, accuracies):\n",
    "    print(f\"Max Depth: {max_depth}, Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Metric  the Classifier  Sklearn Classifier\n",
      "0       Accuracy        0.806154            0.948923\n",
      "1      Precision        0.806111            0.950407\n",
      "2         Recall        0.806154            0.948923\n",
      "3       F1 Score        0.806108            0.948937\n",
      "4  Training Time        0.096362            0.005984\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary classes from sklearn for decision tree classification and various performance metrics.\n",
    "# The DecisionTreeClassifier is essential for building the tree model,\n",
    "# while accuracy_score, precision_score, recall_score, and f1_score are for evaluating its performance.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Including the time library for tracking the training duration of the classifiers.\n",
    "import time\n",
    "# Splitting the dataset into features (X) and labels (y) for the machine learning process.\n",
    "# The features (predictors) are all columns except the first one, and the label (response variable) is the first column.\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "\n",
    "# Dividing the data into training and testing sets to evaluate the model's performance on unseen data.\n",
    "# This is a critical step in validating the effectiveness of the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing an instance of a custom ID3DecisionTreeClassifier with a specified maximum depth of 3.\n",
    "# This classifier is likely a custom implementation of the ID3 algorithm.\n",
    "the_classifier = ID3DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "# Initializing an instance of the scikit-learn DecisionTreeClassifier,\n",
    "# specifying the maximum depth and using 'entropy' as the criterion for splitting.\n",
    "# This classifier represents a standard implementation of decision trees in Python.\n",
    "sklearn_classifier = DecisionTreeClassifier(max_depth=3, criterion='entropy')\n",
    "\n",
    "# Measuring and recording the training time for the custom ID3DecisionTreeClassifier.\n",
    "# Timing starts before training and ends after, calculating the time difference to get the training duration.\n",
    "start_time = time.time()\n",
    "the_classifier.fit(X_train, y_train)\n",
    "the_training_time = time.time() - start_time\n",
    "\n",
    "# Similarly, measuring and recording the training time for the scikit-learn DecisionTreeClassifier.\n",
    "start_time = time.time()\n",
    "sklearn_classifier.fit(X_train, y_train)\n",
    "sklearn_training_time = time.time() - start_time\n",
    "\n",
    "# Generating predictions with the custom classifier for the test data.\n",
    "the_predictions = the_classifier.predict(X_test)\n",
    "\n",
    "# Generating predictions with the sklearn classifier for the test data.\n",
    "sklearn_predictions = sklearn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the performance of the custom classifier using accuracy, precision, recall, and F1 score.\n",
    "# These metrics give a comprehensive view of the model's performance, considering aspects like false positives and negatives.\n",
    "the_accuracy = accuracy_score(y_test, the_predictions)\n",
    "the_precision = precision_score(y_test, the_predictions, average='weighted')\n",
    "the_recall = recall_score(y_test, the_predictions, average='weighted')\n",
    "the_f1 = f1_score(y_test, the_predictions, average='weighted')\n",
    "\n",
    "# Similarly, evaluating the sklearn classifier with the same set of metrics.\n",
    "sklearn_accuracy = accuracy_score(y_test, sklearn_predictions)\n",
    "sklearn_precision = precision_score(y_test, sklearn_predictions, average='weighted')\n",
    "sklearn_recall = recall_score(y_test, sklearn_predictions, average='weighted')\n",
    "sklearn_f1 = f1_score(y_test, sklearn_predictions, average='weighted')\n",
    "\n",
    "# Compiling all results into a pandas DataFrame for a consolidated view.\n",
    "# This includes both classifiers' performance metrics and their training times for comparison.\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Training Time'],\n",
    "    'the Classifier': [the_accuracy, the_precision, the_recall, the_f1, the_training_time],\n",
    "    'Sklearn Classifier': [sklearn_accuracy, sklearn_precision, sklearn_recall, sklearn_f1, sklearn_training_time]\n",
    "})\n",
    "\n",
    "# Printing the results for a quick inspection and comparison between the custom and sklearn classifiers.\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3800\\1847273306.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, the_new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Initializing a DataFrame named 'results_df' with specific columns. \n",
    "# This DataFrame is intended to store various metrics for performance evaluation of classifiers.\n",
    "# These metrics include Accuracy, Precision, Recall, F1 Score, and Training Time, among others.\n",
    "results_df = pd.DataFrame(columns=['Classifier', 'Max_Depth', 'Training_Size', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'Training_Time'])\n",
    "\n",
    "# Setting up arrays for max_depths and training_sizes to iterate through.\n",
    "# These represent different configurations to test the classifiers under various conditions.\n",
    "max_depths = [3, 5, 10]  # Example max_depth values like 3, 5, and 10 to explore varying tree depths.\n",
    "training_sizes = [0.6, 0.7, 0.8]  # Example training sizes (60%, 70%, 80%) to analyze different training set sizes.\n",
    "\n",
    "# Beginning a nested loop: The outer loop iterates over max_depths, and the inner loop over training_sizes.\n",
    "# This double loop is effectively testing all combinations of depths and training sizes.\n",
    "for depth in max_depths:\n",
    "    for size in training_sizes:\n",
    "        # Splitting the dataset into training and testing sets based on the current training size.\n",
    "        # The 'train_test_split' function is being used from scikit-learn, \n",
    "        # ensuring each split has the same random state for consistency.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=size, random_state=42)\n",
    "\n",
    "        # Initializing and training the custom ID3DecisionTreeClassifier with the current depth.\n",
    "        # Also measuring the time taken to train the classifier.\n",
    "        the_classifier = ID3DecisionTreeClassifier(max_depth=depth)\n",
    "        start_time = time.time()\n",
    "        the_classifier.fit(X_train, y_train)\n",
    "        the_training_time = time.time() - start_time\n",
    "\n",
    "        # Predicting and evaluating the performance of the custom classifier on the test set.\n",
    "        # Calculating key metrics such as accuracy, precision, recall, and F1 score.\n",
    "        the_predictions = the_classifier.predict(X_test)\n",
    "        the_accuracy = accuracy_score(y_test, the_predictions)\n",
    "        the_precision = precision_score(y_test, the_predictions, average='weighted')\n",
    "        the_recall = recall_score(y_test, the_predictions, average='weighted')\n",
    "        the_f1 = f1_score(y_test, the_predictions, average='weighted')\n",
    "\n",
    "        # Constructing a new DataFrame row with the results of the custom classifier\n",
    "        # and appending it to the 'results_df' DataFrame.\n",
    "        the_new_row = pd.DataFrame([['theClassifier', depth, size, the_accuracy, the_precision, the_recall, the_f1, the_training_time]], \n",
    "                                   columns=results_df.columns)\n",
    "        results_df = pd.concat([results_df, the_new_row], ignore_index=True)\n",
    "\n",
    "        # Repeating the training and evaluation process for the sklearn DecisionTreeClassifier.\n",
    "        # This classifier uses 'entropy' as the criterion and the same depth as the custom classifier.\n",
    "        sklearn_classifier = DecisionTreeClassifier(max_depth=depth, criterion='entropy')\n",
    "        start_time = time.time()\n",
    "        sklearn_classifier.fit(X_train, y_train)\n",
    "        sklearn_training_time = time.time() - start_time\n",
    "        sklearn_predictions = sklearn_classifier.predict(X_test)\n",
    "        sklearn_accuracy = accuracy_score(y_test, sklearn_predictions)\n",
    "        sklearn_precision = precision_score(y_test, sklearn_predictions, average='weighted')\n",
    "        sklearn_recall = recall_score(y_test, sklearn_predictions, average='weighted')\n",
    "        sklearn_f1 = f1_score(y_test, sklearn_predictions, average='weighted')\n",
    "\n",
    "        # Constructing a new DataFrame row with the results of the sklearn classifier\n",
    "        # and appending it to the 'results_df' DataFrame.\n",
    "        sklearn_new_row = pd.DataFrame([['Sklearn', depth, size, sklearn_accuracy, sklearn_precision, sklearn_recall, sklearn_f1, sklearn_training_time]], \n",
    "                                       columns=results_df.columns)\n",
    "        results_df = pd.concat([results_df, sklearn_new_row], ignore_index=True)\n",
    "\n",
    "# Finally, saving the accumulated results in 'results_df' to a CSV file named 'mushroom_comparison_results.csv'.\n",
    "# The 'index=False' parameter ensures that the DataFrame index is not included in the CSV file.\n",
    "results_df.to_csv('mushroom_comparison_results.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Max_Depth</th>\n",
       "      <th>Training_Size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Training_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.908308</td>\n",
       "      <td>0.911338</td>\n",
       "      <td>0.908308</td>\n",
       "      <td>0.908275</td>\n",
       "      <td>0.140143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.957846</td>\n",
       "      <td>0.958543</td>\n",
       "      <td>0.957846</td>\n",
       "      <td>0.957857</td>\n",
       "      <td>0.003991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.906071</td>\n",
       "      <td>0.908881</td>\n",
       "      <td>0.906071</td>\n",
       "      <td>0.906038</td>\n",
       "      <td>0.081295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.953240</td>\n",
       "      <td>0.954195</td>\n",
       "      <td>0.953240</td>\n",
       "      <td>0.953250</td>\n",
       "      <td>0.003991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.806154</td>\n",
       "      <td>0.806111</td>\n",
       "      <td>0.806154</td>\n",
       "      <td>0.806108</td>\n",
       "      <td>0.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.948923</td>\n",
       "      <td>0.950407</td>\n",
       "      <td>0.948923</td>\n",
       "      <td>0.948937</td>\n",
       "      <td>0.006014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.996031</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.995999</td>\n",
       "      <td>0.210542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984647</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984613</td>\n",
       "      <td>0.003986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.995898</td>\n",
       "      <td>0.995931</td>\n",
       "      <td>0.995898</td>\n",
       "      <td>0.995898</td>\n",
       "      <td>0.198610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.983593</td>\n",
       "      <td>0.983602</td>\n",
       "      <td>0.983593</td>\n",
       "      <td>0.983592</td>\n",
       "      <td>0.003989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.996923</td>\n",
       "      <td>0.996941</td>\n",
       "      <td>0.996923</td>\n",
       "      <td>0.996923</td>\n",
       "      <td>0.210513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.980308</td>\n",
       "      <td>0.981028</td>\n",
       "      <td>0.980308</td>\n",
       "      <td>0.980285</td>\n",
       "      <td>0.006499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.997231</td>\n",
       "      <td>0.997232</td>\n",
       "      <td>0.997231</td>\n",
       "      <td>0.997231</td>\n",
       "      <td>0.206576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.998359</td>\n",
       "      <td>0.998365</td>\n",
       "      <td>0.998359</td>\n",
       "      <td>0.998359</td>\n",
       "      <td>0.222620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>0.999385</td>\n",
       "      <td>0.216594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classifier Max_Depth  Training_Size  Accuracy  Precision    Recall  \\\n",
       "0   theClassifier         3            0.6  0.908308   0.911338  0.908308   \n",
       "1         Sklearn         3            0.6  0.957846   0.958543  0.957846   \n",
       "2   theClassifier         3            0.7  0.906071   0.908881  0.906071   \n",
       "3         Sklearn         3            0.7  0.953240   0.954195  0.953240   \n",
       "4   theClassifier         3            0.8  0.806154   0.806111  0.806154   \n",
       "5         Sklearn         3            0.8  0.948923   0.950407  0.948923   \n",
       "6   theClassifier         5            0.6  0.996000   0.996031  0.996000   \n",
       "7         Sklearn         5            0.6  0.984615   0.984647  0.984615   \n",
       "8   theClassifier         5            0.7  0.995898   0.995931  0.995898   \n",
       "9         Sklearn         5            0.7  0.983593   0.983602  0.983593   \n",
       "10  theClassifier         5            0.8  0.996923   0.996941  0.996923   \n",
       "11        Sklearn         5            0.8  0.980308   0.981028  0.980308   \n",
       "12  theClassifier        10            0.6  0.997231   0.997232  0.997231   \n",
       "13        Sklearn        10            0.6  1.000000   1.000000  1.000000   \n",
       "14  theClassifier        10            0.7  0.998359   0.998365  0.998359   \n",
       "15        Sklearn        10            0.7  1.000000   1.000000  1.000000   \n",
       "16  theClassifier        10            0.8  0.999385   0.999385  0.999385   \n",
       "17        Sklearn        10            0.8  1.000000   1.000000  1.000000   \n",
       "\n",
       "    F1_Score  Training_Time  \n",
       "0   0.908275       0.140143  \n",
       "1   0.957857       0.003991  \n",
       "2   0.906038       0.081295  \n",
       "3   0.953250       0.003991  \n",
       "4   0.806108       0.079300  \n",
       "5   0.948937       0.006014  \n",
       "6   0.995999       0.210542  \n",
       "7   0.984613       0.003986  \n",
       "8   0.995898       0.198610  \n",
       "9   0.983592       0.003989  \n",
       "10  0.996923       0.210513  \n",
       "11  0.980285       0.006499  \n",
       "12  0.997231       0.206576  \n",
       "13  1.000000       0.005965  \n",
       "14  0.998359       0.222620  \n",
       "15  1.000000       0.004987  \n",
       "16  0.999385       0.216594  \n",
       "17  1.000000       0.007000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
