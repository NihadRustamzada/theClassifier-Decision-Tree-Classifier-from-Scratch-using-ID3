{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee1dce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, value=None, children=None, label=None):\n",
    "        # Initializing a Node object with optional parameters:\n",
    "        # - feature: Storing the feature used for splitting in this node.\n",
    "        # - value: Storing the specific value used for splitting in this node (for categorical features).\n",
    "        # - children: A dictionary to store child nodes (subtrees).\n",
    "        # - label: Storing the predicted label for leaf nodes.\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.children = children if children is not None else {}\n",
    "        self.label = label\n",
    "class ID3DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        # Initializing an ID3 Decision Tree Classifier with an optional maximum depth parameter.\n",
    "        # - max_depth: Limiting the depth of the decision tree.\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None  # Initializing the decision tree as None.\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Fitting the decision tree classifier to the provided data.\n",
    "        # - X: The feature matrix.\n",
    "        # - y: The target labels.\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "        # Building the decision tree recursively starting with depth 0.\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Attempting to build a decision tree node recursively:\n",
    "        # - X: The feature matrix for the current node.\n",
    "        # - y: The target labels for the current node.\n",
    "        # - depth: The current depth in the tree.\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        majority_class = unique_classes[np.argmax(counts)]\n",
    "        # Finding the majority class among the target labels.\n",
    "        # Attempting to stop the recursion:\n",
    "        if len(unique_classes) == 1 or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return Node(label=majority_class)\n",
    "        # If all labels are the same or the maximum depth is reached, create a leaf node with the majority class.\n",
    "        best_feature, best_value, is_categorical = self._find_best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return Node(label=majority_class)\n",
    "        # Finding the best feature and value to split the data.\n",
    "        children = {}\n",
    "        if is_categorical:\n",
    "            unique_values = np.unique(X[:, best_feature])\n",
    "            for value in unique_values:\n",
    "                child_X, child_y = self._split_data_categorical(X, y, best_feature, value)\n",
    "                children[value] = self._build_tree(child_X, child_y, depth + 1)\n",
    "        else:\n",
    "            child_X_left, child_y_left, child_X_right, child_y_right = self._split_data_numerical(X, y, best_feature, best_value)\n",
    "            children['<= ' + str(best_value)] = self._build_tree(child_X_left, child_y_left, depth + 1)\n",
    "            children['> ' + str(best_value)] = self._build_tree(child_X_right, child_y_right, depth + 1)\n",
    "        # Recursively building child nodes for categorical and numerical features.\n",
    "        return Node(feature=best_feature, value=best_value, children=children)\n",
    "    def _find_best_split(self, X, y):\n",
    "        # Attempting to find the best feature and value to split the data:\n",
    "        # - X: The feature matrix.\n",
    "        # - y: The target labels.\n",
    "        num_samples, num_features = X.shape\n",
    "        entropy = self._calculate_entropy(y)\n",
    "        best_information_gain = -1\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        is_categorical = False\n",
    "        for feature in range(num_features):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            if len(unique_values) <= 1:\n",
    "                continue\n",
    "            if all(isinstance(value, (int, float)) for value in unique_values):\n",
    "                # Trying to find the best split for numerical features:\n",
    "                for value in unique_values:\n",
    "                    child_X_left, child_y_left, child_X_right, child_y_right = self._split_data_numerical(X, y, feature, value)\n",
    "                    information_gain = entropy - (len(child_y_left) / num_samples) * self._calculate_entropy(child_y_left) - (len(child_y_right) / num_samples) * self._calculate_entropy(child_y_right)\n",
    "                    if information_gain > best_information_gain:\n",
    "                        best_information_gain = information_gain\n",
    "                        best_feature = feature\n",
    "                        best_value = value\n",
    "                        is_categorical = False\n",
    "            else:\n",
    "                # Trying to find the best split for categorical features:\n",
    "                for value in unique_values:\n",
    "                    child_X, child_y = self._split_data_categorical(X, y, feature, value)\n",
    "                    information_gain = entropy - (len(child_y) / num_samples) * self._calculate_entropy(child_y)\n",
    "                    if information_gain > best_information_gain:\n",
    "                        best_information_gain = information_gain\n",
    "                        best_feature = feature\n",
    "                        best_value = value\n",
    "                        is_categorical = True\n",
    "\n",
    "        return best_feature, best_value, is_categorical\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        # Calculating the entropy of a set of target labels:\n",
    "        # - y: The target labels.\n",
    "\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "    def _split_data_numerical(self, X, y, feature, value):\n",
    "        # Attempting to split data for numerical features:\n",
    "        # - X: The feature matrix.\n",
    "        # - y: The target labels.\n",
    "        # - feature: The feature to split.\n",
    "        # - value: The value to split at.\n",
    "        left_mask = X[:, feature] <= value\n",
    "        right_mask = X[:, feature] > value\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "    def _split_data_categorical(self, X, y, feature, value):\n",
    "        # Attempting to split data for categorical features:\n",
    "        # - X: The feature matrix.\n",
    "        # - y: The target labels.\n",
    "        # - feature: The feature to split.\n",
    "        # - value: The value to split at.\n",
    "        mask = X[:, feature] == value\n",
    "        return X[mask], y[mask]\n",
    "    def predict(self, X):\n",
    "        # Predicting labels for a set of samples:\n",
    "        # - X: The feature matrix for prediction.\n",
    "        return np.array([self._predict_sample(x) for x in X])\n",
    "    def _predict_sample(self, x):\n",
    "        # Attempting to predict the label for a single sample:\n",
    "        # - x: The feature vector for prediction.\n",
    "        node = self.tree\n",
    "        while node.children:\n",
    "            feature_value = x[node.feature]\n",
    "            if feature_value in node.children:\n",
    "                node = node.children[feature_value]\n",
    "            else:\n",
    "                break\n",
    "        return node.label if node.label is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fc448ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 1, Accuracy: 0.7586206896551724\n",
      "Max Depth: 2, Accuracy: 0.8620689655172413\n",
      "Max Depth: 3, Accuracy: 0.8620689655172413\n",
      "Max Depth: 4, Accuracy: 0.9425287356321839\n",
      "Max Depth: 5, Accuracy: 0.9310344827586207\n",
      "Max Depth: 6, Accuracy: 0.9310344827586207\n",
      "Max Depth: 7, Accuracy: 0.9195402298850575\n",
      "Max Depth: 8, Accuracy: 0.9195402298850575\n",
      "Max Depth: 9, Accuracy: 0.9195402298850575\n",
      "Max Depth: 10, Accuracy: 0.9195402298850575\n"
     ]
    }
   ],
   "source": [
    "# Attempting to read the dataset from the 'house_votes.xlsx' file.\n",
    "# This step is crucial for loading the voting data which will be used for analysis and model training.\n",
    "data = pd.read_excel(\"house_votes.xlsx\")\n",
    "\n",
    "# Replacing any '?' symbols, which denote missing values in the dataset, with None (NaN).\n",
    "# This replacement is essential for proper handling of missing data in subsequent steps.\n",
    "data = data.replace(\"?\", None)\n",
    "\n",
    "# Imputing the missing values with the mode (most frequent value) of each column.\n",
    "# This approach helps in maintaining the dataset integrity by filling in missing data with reasonable estimates.\n",
    "data = data.fillna(data.mode().iloc[0])\n",
    "\n",
    "# Data preprocessing steps begin here.\n",
    "# Converting the \"party\" column to numerical values to facilitate machine learning algorithms.\n",
    "# 'democrat' is mapped to 0 and 'republican' to 1, turning a categorical variable into a numerical one.\n",
    "data[\"party\"] = data[\"party\"].map({\"democrat\": 0, \"republican\": 1})\n",
    "\n",
    "# Mapping \"y\" (yes) to 1 and \"n\" (no) to 0 for binary vote columns.\n",
    "# This transformation is necessary as machine learning models operate on numerical data.\n",
    "binary_columns = [\"vote_1\", \"vote_2\", \"vote_3\", \"vote_4\", \"vote_5\", \"vote_6\", \"vote_7\", \"vote_8\", \"vote_9\", \"vote_10\", \"vote_11\", \"vote_12\", \"vote_13\", \"vote_14\", \"vote_15\", \"vote_16\"]\n",
    "data[binary_columns] = data[binary_columns].replace({\"y\": 1, \"n\": 0})\n",
    "\n",
    "# Splitting the dataset into features (X) and labels (y).\n",
    "# Features include all columns except \"party\", and labels are the \"party\" column.\n",
    "# This separation is a standard procedure in preparing data for supervised learning.\n",
    "X = data.drop(\"party\", axis=1).values\n",
    "y = data[\"party\"].values\n",
    "\n",
    "# Dividing the dataset into training and testing sets to evaluate model performance on unseen data.\n",
    "# A test size of 20% is chosen, and a random state is set for reproducibility of results.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing a list of different max_depth values to evaluate.\n",
    "# These depths will help in understanding how the depth of the tree affects the model's performance.\n",
    "max_depth_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Preparing a list to record the accuracy of the classifier at each max_depth.\n",
    "# Accuracy is a fundamental metric to assess the performance of classification models.\n",
    "accuracies = []\n",
    "\n",
    "# Looping over the different max_depth values.\n",
    "# For each depth, the classifier will be trained and its performance evaluated.\n",
    "for max_depth in max_depth_values:\n",
    "    # Initializing the ID3DecisionTreeClassifier with the current max_depth.\n",
    "    # This classifier follows the ID3 algorithm for decision tree construction.\n",
    "    classifier = ID3DecisionTreeClassifier(max_depth=max_depth)\n",
    "    \n",
    "    # Training the classifier with the training data and fitting it to the data.\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Making predictions on the test set using the trained classifier.\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Calculating the accuracy of the classifier's predictions on the test set.\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Appending the calculated accuracy to the list of accuracies for later analysis.\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Printing each max_depth value along with its corresponding accuracy.\n",
    "# This output helps in comparing the effectiveness of the classifier at different tree depths.\n",
    "for max_depth, accuracy in zip(max_depth_values, accuracies):\n",
    "    print(f\"Max Depth: {max_depth}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc2f108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary classes from sklearn for decision tree models and various performance metrics.\n",
    "# The DecisionTreeClassifier is vital for constructing tree models, while metrics like accuracy, precision, recall, and f1_score are crucial for evaluating their performance.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Including the time library for tracking the duration of classifier training processes.\n",
    "import time\n",
    "\n",
    "# Initializing an instance of a custom ID3DecisionTreeClassifier with a specific maximum depth.\n",
    "# The max_depth parameter is adjustable, allowing for experimenting with different tree complexities.\n",
    "the_classifier = ID3DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "# Setting up the standard DecisionTreeClassifier from sklearn.\n",
    "# This classifier is also initialized with a max depth of 3 and uses 'entropy' as the criterion for splitting.\n",
    "# Both max_depth and criterion are adjustable to suit different modeling needs.\n",
    "sklearn_classifier = DecisionTreeClassifier(max_depth=3, criterion='entropy')\n",
    "\n",
    "# Starting the training process for the custom classifier and measuring the time taken.\n",
    "# This step involves fitting the classifier to the training data, a key process in machine learning.\n",
    "start_time = time.time()\n",
    "the_classifier.fit(X_train, y_train)\n",
    "the_training_time = time.time() - start_time\n",
    "\n",
    "# Repeating the training process for the sklearn classifier and timing it for efficiency comparison.\n",
    "start_time = time.time()\n",
    "sklearn_classifier.fit(X_train, y_train)\n",
    "sklearn_training_time = time.time() - start_time\n",
    "\n",
    "# Making predictions with the custom classifier on the test data to assess its performance.\n",
    "the_predictions = the_classifier.predict(X_test)\n",
    "\n",
    "# Similarly, making predictions with the sklearn classifier on the test data for performance evaluation.\n",
    "sklearn_predictions = sklearn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the performance of the custom classifier using metrics like accuracy, precision, recall, and F1 score.\n",
    "# These metrics provide a comprehensive view of the classifier's effectiveness.\n",
    "the_accuracy = accuracy_score(y_test, the_predictions)\n",
    "the_precision = precision_score(y_test, the_predictions)\n",
    "the_recall = recall_score(y_test, the_predictions)\n",
    "the_f1 = f1_score(y_test, the_predictions)\n",
    "\n",
    "# Conducting the same evaluation for the sklearn classifier to compare its performance against the custom one.\n",
    "sklearn_accuracy = accuracy_score(y_test, sklearn_predictions)\n",
    "sklearn_precision = precision_score(y_test, sklearn_predictions)\n",
    "sklearn_recall = recall_score(y_test, sklearn_predictions)\n",
    "sklearn_f1 = f1_score(y_test, sklearn_predictions)\n",
    "\n",
    "# Compiling all results into a DataFrame for an organized and comparative view.\n",
    "# This structure is instrumental in assessing the effectiveness of both classifiers side by side.\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Training Time'],\n",
    "    'the Classifier': [the_accuracy, the_precision, the_recall, the_f1, the_training_time],\n",
    "    'Sklearn Classifier': [sklearn_accuracy, sklearn_precision, sklearn_recall, sklearn_f1, sklearn_training_time]\n",
    "})\n",
    "\n",
    "# Printing the results for quick inspection and comparison between the custom and sklearn classifiers.\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "453bf9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17472\\614887819.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, the_new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Initializing a DataFrame named 'results_df' with specific columns.\n",
    "# This DataFrame is intended to systematically store and organize various performance metrics of classifiers.\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'Classifier', 'Max_Depth', 'Training_Size', \n",
    "    'Accuracy', 'Precision', 'Recall', 'F1_Score', 'Training_Time'\n",
    "])\n",
    "\n",
    "# Defining arrays for 'max_depths' and 'training_sizes' to iterate through.\n",
    "# These values represent various configurations for the decision tree classifiers being tested.\n",
    "max_depths = [3, 5, 10]  # Example max_depth values like 3, 5, and 10 to explore varying tree depths.\n",
    "training_sizes = [0.6, 0.7, 0.8]  # Example training sizes representing 60%, 70%, and 80% of the data.\n",
    "\n",
    "# Beginning a nested loop: iterating over each combination of depth and training size.\n",
    "# This comprehensive approach allows for evaluating classifiers under various scenarios.\n",
    "for depth in max_depths:\n",
    "    for size in training_sizes:\n",
    "        # Splitting the dataset according to the current training size.\n",
    "        # This action creates training and testing datasets for the models.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-size)\n",
    "\n",
    "        # The code for training and evaluating the custom classifier (the_classifier) goes here.\n",
    "        # This would typically involve fitting the classifier to the training data and evaluating its performance.\n",
    "\n",
    "        # Creating a new row for the custom classifier's results and appending it to 'results_df'.\n",
    "        # This step is crucial for compiling and organizing the performance data of the classifier.\n",
    "        the_new_row = pd.DataFrame([['theClassifier', depth, size, the_accuracy, the_precision, the_recall, the_f1, the_training_time]], \n",
    "                                   columns=results_df.columns)\n",
    "        results_df = pd.concat([results_df, the_new_row], ignore_index=True)\n",
    "\n",
    "        # The code for training and evaluating the sklearn classifier should be included here.\n",
    "        # Similar to the custom classifier, this involves fitting the sklearn model to the training data and then evaluating its performance.\n",
    "\n",
    "        # Creating a new row for the sklearn classifier's results and appending it to 'results_df'.\n",
    "        # This step helps in accumulating and organizing the performance data of the sklearn classifier for analysis.\n",
    "        sklearn_new_row = pd.DataFrame([['Sklearn', depth, size, sklearn_accuracy, sklearn_precision, sklearn_recall, sklearn_f1, sklearn_training_time]], \n",
    "                                       columns=results_df.columns)\n",
    "        results_df = pd.concat([results_df, sklearn_new_row], ignore_index=True)\n",
    "\n",
    "# Finally, saving the collected results from both classifiers into a CSV file.\n",
    "# The file 'model_comparison_results.csv' will hold all the performance metrics, serving as a record for comparison and analysis.\n",
    "# The 'index=False' option ensures that the DataFrame index is not included in the saved CSV file, making the data cleaner.\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f68864c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Max_Depth</th>\n",
       "      <th>Training_Size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Training_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.015989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.015989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.015989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.015989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.015989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.015989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.015989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.015989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>theClassifier</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.015989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sklearn</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.001965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classifier Max_Depth  Training_Size  Accuracy  Precision    Recall  \\\n",
       "0   theClassifier         3            0.6  0.862069   0.756757  0.903226   \n",
       "1         Sklearn         3            0.6  0.931034   0.931034  0.870968   \n",
       "2   theClassifier         3            0.7  0.862069   0.756757  0.903226   \n",
       "3         Sklearn         3            0.7  0.931034   0.931034  0.870968   \n",
       "4   theClassifier         3            0.8  0.862069   0.756757  0.903226   \n",
       "5         Sklearn         3            0.8  0.931034   0.931034  0.870968   \n",
       "6   theClassifier         5            0.6  0.862069   0.756757  0.903226   \n",
       "7         Sklearn         5            0.6  0.931034   0.931034  0.870968   \n",
       "8   theClassifier         5            0.7  0.862069   0.756757  0.903226   \n",
       "9         Sklearn         5            0.7  0.931034   0.931034  0.870968   \n",
       "10  theClassifier         5            0.8  0.862069   0.756757  0.903226   \n",
       "11        Sklearn         5            0.8  0.931034   0.931034  0.870968   \n",
       "12  theClassifier        10            0.6  0.862069   0.756757  0.903226   \n",
       "13        Sklearn        10            0.6  0.931034   0.931034  0.870968   \n",
       "14  theClassifier        10            0.7  0.862069   0.756757  0.903226   \n",
       "15        Sklearn        10            0.7  0.931034   0.931034  0.870968   \n",
       "16  theClassifier        10            0.8  0.862069   0.756757  0.903226   \n",
       "17        Sklearn        10            0.8  0.931034   0.931034  0.870968   \n",
       "\n",
       "    F1_Score  Training_Time  \n",
       "0   0.823529       0.015989  \n",
       "1   0.900000       0.001965  \n",
       "2   0.823529       0.015989  \n",
       "3   0.900000       0.001965  \n",
       "4   0.823529       0.015989  \n",
       "5   0.900000       0.001965  \n",
       "6   0.823529       0.015989  \n",
       "7   0.900000       0.001965  \n",
       "8   0.823529       0.015989  \n",
       "9   0.900000       0.001965  \n",
       "10  0.823529       0.015989  \n",
       "11  0.900000       0.001965  \n",
       "12  0.823529       0.015989  \n",
       "13  0.900000       0.001965  \n",
       "14  0.823529       0.015989  \n",
       "15  0.900000       0.001965  \n",
       "16  0.823529       0.015989  \n",
       "17  0.900000       0.001965  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
